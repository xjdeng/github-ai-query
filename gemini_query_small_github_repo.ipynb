{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMCWvBqxeDsw6/viAGr+Zr1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjdeng/github-ai-query/blob/main/gemini_query_small_github_repo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install path.py==12.0.1\n",
        "# Import necessary modules\n",
        "import os\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "from path import Path as path\n",
        "from vertexai.preview import tokenization\n",
        "import pprint\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "model = genai.GenerativeModel(\"gemini-1.5-flash-latest\")\n",
        "model_name = \"gemini-1.5-flash\"\n",
        "tokenizer = tokenization.get_tokenizer_for_model(model_name)\n",
        "\n",
        "def count_tokens(txt):\n",
        "  result = tokenizer.count_tokens(txt)\n",
        "  return result.total_tokens\n",
        "\n",
        "def generate_tree(directory, prefix=\"\"):\n",
        "\n",
        "    tree = []\n",
        "    entries = sorted(os.listdir(directory))  # Sort entries for consistent order\n",
        "\n",
        "    for index, entry in enumerate(entries):\n",
        "        path = os.path.join(directory, entry)\n",
        "        is_last = (index == len(entries) - 1)\n",
        "\n",
        "        if is_last:\n",
        "            tree.append(f\"{prefix}└── {entry}\")\n",
        "            next_prefix = f\"{prefix}    \"\n",
        "        else:\n",
        "            tree.append(f\"{prefix}├── {entry}\")\n",
        "            next_prefix = f\"{prefix}│   \"\n",
        "\n",
        "        if os.path.isdir(path):\n",
        "            tree.append(generate_tree(path, next_prefix))\n",
        "    return \"\\n\".join(tree)\n",
        "\n",
        "def query_repo(query, repo_url, refresh = False):\n",
        "  if repo_url.lower().endswith(\".git\"):\n",
        "    repo_url = repo_url[:-4]\n",
        "  local_repo_dir = str(path(repo_url).name)\n",
        "  if refresh:\n",
        "    !rm -rf $local_repo_dir\n",
        "  if not path(local_repo_dir).exists():\n",
        "    !git clone $repo_url\n",
        "  files_to_open = []\n",
        "  for f in path(local_repo_dir).walkfiles():\n",
        "    if (f.ext == \".py\") or (f.ext == \".md\") or (f.ext == \".txt\"):\n",
        "      files_to_open.append(f)\n",
        "  prompt = f\"\"\"\n",
        "  I'm making the following query on the Github repo below and I'll give you the file structure and the contents of all of the files.\n",
        "\n",
        "  My Query:\n",
        "  ---\n",
        "  {query}\n",
        "  ---\n",
        "\n",
        "  File structure:\n",
        "  ---\n",
        "  {generate_tree(local_repo_dir)}\n",
        "  ---\n",
        "\n",
        "  Next, I'll give you the contents of each file in the following section -\n",
        "  *************************************************************************\n",
        "\n",
        "  \"\"\"\n",
        "  for f in files_to_open:\n",
        "    with open(f,'r') as ff:\n",
        "      contents = ff.read()\n",
        "    prompt += f\"\"\"\n",
        "    {str(f.name)}:\n",
        "    ---\n",
        "    {contents}\n",
        "    ---\n",
        "\n",
        "    \"\"\"\n",
        "  prompt += \"*************************************************************************\"\n",
        "  n_tokens = count_tokens(prompt)\n",
        "  print(f\"{n_tokens} Tokens\")\n",
        "  if n_tokens > 1000000:\n",
        "    print(\"This repo has more than 1 million tokens and the query is likely to fail as a result due to Gemini's context window of 1 million.\")\n",
        "  response = model.generate_content(prompt)\n",
        "  return response.text"
      ],
      "metadata": {
        "id": "eEaM9XrLhzVB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint.pprint(query_repo(\"Do a code review on this repo and point out its biggest problems.\",\n",
        "                         \"https://github.com/fastai/fastai\"))"
      ],
      "metadata": {
        "id": "_CsIFjmC_mfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}